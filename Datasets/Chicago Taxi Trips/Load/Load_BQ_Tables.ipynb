{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Pickled Dataframes into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "directory = '../.pickles'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.pkl'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, 'rb') as file:\n",
    "            df_name = os.path.splitext(filename)[0]\n",
    "            globals()[df_name] = pd.DataFrame(pickle.load(file))\n",
    "\n",
    "# Now each pickle file is loaded into its own respective DataFrame variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a connection to Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets in project mikecancell-development:\n",
      "\tDatasets\n",
      "\tVC_data_job_postings_data_api\n",
      "\tuber_data\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Path to the service account key file\n",
    "key_path = \"/Users/mike/Develop/Conf/GCP Service Keys/mikecancell-development-0bcca41f8486.json\"\n",
    "\n",
    "# Create credentials using the service account key file\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "\n",
    "# Create a BigQuery client using the credentials\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "# Test the connection by listing datasets\n",
    "datasets = list(client.list_datasets())\n",
    "if datasets:\n",
    "    print(\"Datasets in project {}:\".format(client.project))\n",
    "    for dataset in datasets:\n",
    "        print(\"\\t{}\".format(dataset.dataset_id))\n",
    "else:\n",
    "    print(\"{} project does not contain any datasets.\".format(client.project))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset to stored the tables (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset mikecancell-development.Datasets already exists.\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset ID\n",
    "dataset_id = \"{}.Datasets\".format(client.project)\n",
    "\n",
    "# Check if the dataset already exists\n",
    "try:\n",
    "\tclient.get_dataset(dataset_id)  # Make an API request.\n",
    "\tprint(\"Dataset {} already exists.\".format(dataset_id))\n",
    "except Exception:\n",
    "\t# Construct a full Dataset object to send to the API\n",
    "\tdataset = bigquery.Dataset(dataset_id)\n",
    "\n",
    "\t# Specify the geographic location where the dataset should reside\n",
    "\tdataset.location = \"US\"\n",
    "\n",
    "\t# Send the dataset to the API for creation\n",
    "\tdataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "\n",
    "\tprint(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Tables to the Clould"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load to Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 9137.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table mikecancell-development.Datasets.dim_location\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24672.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table mikecancell-development.Datasets.dim_payment_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21845.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table mikecancell-development.Datasets.dim_taxi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11459.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table mikecancell-development.Datasets.dim_time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24818.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table mikecancell-development.Datasets.fact_trips\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12192.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table mikecancell-development.Datasets.grouped_fact_trips_by_wk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Install the pandas-gbq package\n",
    "# %pip install pandas-gbq\n",
    "\n",
    "from pandas_gbq import to_gbq\n",
    "\n",
    "# Define the dataset ID\n",
    "dataset_id = \"Datasets\"\n",
    "\n",
    "# Define the table names\n",
    "table_names = {\n",
    "    \"dim_location\": \"dim_location\",\n",
    "    \"dim_payment_type\": \"dim_payment_type\",\n",
    "    \"dim_taxi\": \"dim_taxi\",\n",
    "    \"dim_time\": \"dim_time\",\n",
    "    \"fact_trips\": \"fact_trips\",\n",
    "    \"grouped_fact_trips_by_wk\": \"grouped_fact_trips_by_wk\"\n",
    "}\n",
    "\n",
    "# Define the dataframes\n",
    "dataframes = {\n",
    "    \"dim_location\": dim_location,\n",
    "    \"dim_payment_type\": dim_payment_type,\n",
    "    \"dim_taxi\": dim_taxi,\n",
    "    \"dim_time\": dim_time,\n",
    "    \"fact_trips\": fact_trips,\n",
    "    \"grouped_fact_trips_by_wk\": grouped_fact_trips_by_wk\n",
    "}\n",
    "\n",
    "# Create tables in BigQuery\n",
    "for df_name, table_name in table_names.items():\n",
    "    table_id = f\"{client.project}.{dataset_id}.{table_name}\"\n",
    "    to_gbq(dataframes[df_name], table_id, project_id=client.project, if_exists='replace', credentials=credentials)\n",
    "    print(f\"Created table {table_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Delete the Tables from BQ to avoid cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted table mikecancell-development.Datasets.dim_location\n",
      "Deleted table mikecancell-development.Datasets.dim_payment_type\n",
      "Deleted table mikecancell-development.Datasets.dim_taxi\n",
      "Deleted table mikecancell-development.Datasets.dim_time\n",
      "Deleted table mikecancell-development.Datasets.fact_trips\n",
      "Deleted table mikecancell-development.Datasets.grouped_fact_trips_by_wk\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset ID\n",
    "dataset_id = \"Datasets\"\n",
    "\n",
    "# Define the table names\n",
    "table_names = [\n",
    "    \"dim_location\",\n",
    "    \"dim_payment_type\",\n",
    "    \"dim_taxi\",\n",
    "    \"dim_time\",\n",
    "    \"fact_trips\",\n",
    "    \"grouped_fact_trips_by_wk\"\n",
    "]\n",
    "\n",
    "# Drop tables in BigQuery\n",
    "for table_name in table_names:\n",
    "    table_id = f\"{client.project}.{dataset_id}.{table_name}\"\n",
    "    client.delete_table(table_id, not_found_ok=True)  # Make an API request.\n",
    "    print(f\"Deleted table {table_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done for Now!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
