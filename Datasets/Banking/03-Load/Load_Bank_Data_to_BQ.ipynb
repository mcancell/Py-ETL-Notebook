{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Conf and Credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Directory Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Common_Funcs_Dir': '/Users/mike/Develop/Projects/Code Notebook/Common/Functions', 'Credentials_Dir': '/Users/mike/Develop/Projects/Code Notebook/Credentials', 'Rel_Pickes_Dir': '../.pickles', 'Pub_Data_Dir': \"'/Users/mike/Data/Public\", 'BQ_Service_Key': '/Users/mike/Develop/Conf/GCP Service Keys/mikecancell-development-0bcca41f8486.json'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Check if the file exists and load the JSON file into a dictionary\n",
    "file_path = r'C:\\Users\\mike\\Develop\\Projects\\Code Notebook\\Credentials\\locations_conf.json'\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        locations_data = json.load(f)\n",
    "    print(locations_data)\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Pickled Dataframes into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DataFrame from zip: dim_age\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45 entries, 0 to 44\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype   \n",
      "---  ------     --------------  -----   \n",
      " 0   Age        45 non-null     int64   \n",
      " 1   Age_Group  45 non-null     category\n",
      "dtypes: category(1), int64(1)\n",
      "memory usage: 601.0 bytes\n",
      "None\n",
      "Loaded DataFrame from zip: dim_contact\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 2 columns):\n",
      " #   Column                Non-Null Count  Dtype   \n",
      "---  ------                --------------  -----   \n",
      " 0   Contacts_Count        7 non-null      int64   \n",
      " 1   Contacts_Count_Group  7 non-null      category\n",
      "dtypes: category(1), int64(1)\n",
      "memory usage: 227.0 bytes\n",
      "None\n",
      "Loaded DataFrame from zip: dim_credit_limits\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6205 entries, 0 to 6204\n",
      "Data columns (total 2 columns):\n",
      " #   Column              Non-Null Count  Dtype   \n",
      "---  ------              --------------  -----   \n",
      " 0   Credit_Limit        6205 non-null   float64 \n",
      " 1   Credit_Limit_Group  6205 non-null   category\n",
      "dtypes: category(1), float64(1)\n",
      "memory usage: 54.7 KB\n",
      "None\n",
      "Loaded DataFrame from zip: dim_inactive\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 2 columns):\n",
      " #   Column                 Non-Null Count  Dtype   \n",
      "---  ------                 --------------  -----   \n",
      " 0   Months_Inactive        7 non-null      int64   \n",
      " 1   Months_Inactive_Group  7 non-null      category\n",
      "dtypes: category(1), int64(1)\n",
      "memory usage: 227.0 bytes\n",
      "None\n",
      "Loaded DataFrame from zip: dim_naive_bayes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1591 entries, 0 to 1590\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype   \n",
      "---  ------             --------------  -----   \n",
      " 0   Naive_Bayes        1591 non-null   float64 \n",
      " 1   Naive_Bayes_Group  1591 non-null   category\n",
      "dtypes: category(1), float64(1)\n",
      "memory usage: 14.1 KB\n",
      "None\n",
      "Loaded DataFrame from zip: dim_revolving_bal\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1974 entries, 0 to 1973\n",
      "Data columns (total 2 columns):\n",
      " #   Column               Non-Null Count  Dtype   \n",
      "---  ------               --------------  -----   \n",
      " 0   Total_Revolving_Bal  1974 non-null   int64   \n",
      " 1   Revolving_Bal_Group  1974 non-null   category\n",
      "dtypes: category(1), int64(1)\n",
      "memory usage: 17.5 KB\n",
      "None\n",
      "Loaded DataFrame from zip: dim_trans_amt\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5033 entries, 0 to 5032\n",
      "Data columns (total 2 columns):\n",
      " #   Column                 Non-Null Count  Dtype   \n",
      "---  ------                 --------------  -----   \n",
      " 0   Total_Trans_Amt        5033 non-null   int64   \n",
      " 1   Total_Trans_Amt_Group  5033 non-null   category\n",
      "dtypes: category(1), int64(1)\n",
      "memory usage: 44.4 KB\n",
      "None\n",
      "Loaded DataFrame from zip: dim_trans_cnt\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 126 entries, 0 to 125\n",
      "Data columns (total 2 columns):\n",
      " #   Column                Non-Null Count  Dtype   \n",
      "---  ------                --------------  -----   \n",
      " 0   Total_Trans_Ct        126 non-null    int64   \n",
      " 1   Total_Trans_Ct_Group  126 non-null    category\n",
      "dtypes: category(1), int64(1)\n",
      "memory usage: 1.3 KB\n",
      "None\n",
      "Loaded DataFrame from zip: dim_utilization\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 964 entries, 0 to 963\n",
      "Data columns (total 2 columns):\n",
      " #   Column                   Non-Null Count  Dtype   \n",
      "---  ------                   --------------  -----   \n",
      " 0   Avg_Utilization_Ratio    964 non-null    float64 \n",
      " 1   Utilization_Ratio_Group  964 non-null    category\n",
      "dtypes: category(1), float64(1)\n",
      "memory usage: 8.6 KB\n",
      "None\n",
      "Loaded DataFrame from zip: fact_table\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10127 entries, 0 to 10126\n",
      "Data columns (total 21 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Attrition_Flag            10127 non-null  object \n",
      " 1   Age                       10127 non-null  int64  \n",
      " 2   Gender                    10127 non-null  object \n",
      " 3   Dependent_count           10127 non-null  int64  \n",
      " 4   Education_Level           10127 non-null  object \n",
      " 5   Marital_Status            10127 non-null  object \n",
      " 6   Income_Category           10127 non-null  object \n",
      " 7   Card_Category             10127 non-null  object \n",
      " 8   Months_on_book            10127 non-null  int64  \n",
      " 9   Total_Relationship_Count  10127 non-null  int64  \n",
      " 10  Months_Inactive_12_mon    10127 non-null  int64  \n",
      " 11  Contacts_Count_12_mon     10127 non-null  int64  \n",
      " 12  Credit_Limit              10127 non-null  float64\n",
      " 13  Total_Revolving_Bal       10127 non-null  int64  \n",
      " 14  Avg_Open_To_Buy           10127 non-null  float64\n",
      " 15  Total_Amt_Chng_Q4_Q1      10127 non-null  float64\n",
      " 16  Total_Trans_Amt           10127 non-null  int64  \n",
      " 17  Total_Trans_Ct            10127 non-null  int64  \n",
      " 18  Total_Ct_Chng_Q4_Q1       10127 non-null  float64\n",
      " 19  Avg_Utilization_Ratio     10127 non-null  float64\n",
      " 20  is_Attrited               10127 non-null  bool   \n",
      "dtypes: bool(1), float64(5), int64(9), object(6)\n",
      "memory usage: 1.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "directory = locations_data['Rel_Pickes_Dir']\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.pkl') or filename.endswith('.pkl.zip'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        try:\n",
    "            if filename.endswith('.pkl.zip'):\n",
    "                with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "                    for zip_info in zip_ref.infolist():\n",
    "                        if zip_info.filename.endswith('.pkl'):\n",
    "                            with zip_ref.open(zip_info) as file:\n",
    "                                df_name = os.path.splitext(os.path.splitext(filename)[0])[0]\n",
    "                                globals()[df_name] = pd.DataFrame(pickle.load(file))\n",
    "                                print(f\"Loaded DataFrame from zip: {df_name}\")\n",
    "                                print(globals()[df_name].info())\n",
    "            else:\n",
    "                with open(filepath, 'rb') as file:\n",
    "                    df_name = os.path.splitext(filename)[0]\n",
    "                    globals()[df_name] = pd.DataFrame(pickle.load(file))\n",
    "                    print(f\"Loaded DataFrame: {df_name}\")\n",
    "                    print(globals()[df_name].info())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "\n",
    "# Now each pickle file is loaded into its own respective DataFrame variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a connection to Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets in project mikecancell-development:\n",
      "\tDatasets\n",
      "\tVC_data_job_postings_data_api\n",
      "\tdata_commons\n",
      "\tuber_data\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Path to the service account key file\n",
    "key_path = locations_data['BQ_Service_Key']\n",
    "\n",
    "# Create credentials using the service account key file\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "\n",
    "# Create a BigQuery client using the credentials\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "# Test the connection by listing datasets\n",
    "datasets = list(client.list_datasets())\n",
    "if datasets:\n",
    "    print(\"Datasets in project {}:\".format(client.project))\n",
    "    for dataset in datasets:\n",
    "        print(\"\\t{}\".format(dataset.dataset_id))\n",
    "else:\n",
    "    print(\"{} project does not contain any datasets.\".format(client.project))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset to Store the Tables (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset mikecancell-development.Datasets already exists.\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset ID\n",
    "dataset_id = \"{}.Datasets\".format(client.project)\n",
    "\n",
    "# Check if the dataset already exists\n",
    "try:\n",
    "\tclient.get_dataset(dataset_id)  # Make an API request.\n",
    "\tprint(\"Dataset {} already exists.\".format(dataset_id))\n",
    "except Exception:\n",
    "\t# Construct a full Dataset object to send to the API\n",
    "\tdataset = bigquery.Dataset(dataset_id)\n",
    "\n",
    "\t# Specify the geographic location where the dataset should reside\n",
    "\tdataset.location = \"US\"\n",
    "\n",
    "\t# Send the dataset to the API for creation\n",
    "\tdataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "\n",
    "\tprint(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Tables to the Clould"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure the BQ Libs are Loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from google.cloud import bigquery\n",
    "import google.api_core.exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Dataframe Names to Banking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update table names to precede with \"Banking_Data_\"\n",
    "table_names = {\n",
    "    'fact_table': 'Banking_Data_fact_table',\n",
    "    'dim_age': 'Banking_Data_dim_age',\n",
    "    'dim_contact': 'Banking_Data_dim_contact',\n",
    "    'dim_credit_limits': 'Banking_Data_dim_credit_limits',\n",
    "    'dim_inactive': 'Banking_Data_dim_inactive',\n",
    "    'dim_naive_bayes': 'Banking_Data_dim_naive_bayes',\n",
    "    'dim_revolving_bal': 'Banking_Data_dim_revolving_bal',\n",
    "    'dim_trans_amt': 'Banking_Data_dim_trans_amt',\n",
    "    'dim_trans_cnt': 'Banking_Data_dim_trans_cnt',\n",
    "    'dim_utilization': 'Banking_Data_dim_utilization'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Data Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the DataFrames and their corresponding BigQuery table names\n",
    "dataframes = {\n",
    "    'fact_table': fact_table,\n",
    "    'dim_age': dim_age,\n",
    "    'dim_contact': dim_contact,\n",
    "    'dim_credit_limits': dim_credit_limits,\n",
    "    'dim_inactive': dim_inactive,\n",
    "    'dim_naive_bayes': dim_naive_bayes,\n",
    "    'dim_revolving_bal': dim_revolving_bal,\n",
    "    'dim_trans_amt': dim_trans_amt,\n",
    "    'dim_trans_cnt': dim_trans_cnt,\n",
    "    'dim_utilization': dim_utilization\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Data Frame tyes to Appropriate BQ Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns containing lists to strings\n",
    "for df_name, df in dataframes.items():\n",
    "    for column in df.columns:\n",
    "        if df[column].apply(lambda x: isinstance(x, list)).any():\n",
    "            df[column] = df[column].apply(lambda x: str(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Ensure all columns have explicit data types\n",
    "for df_name, df in dataframes.items():\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            df[column] = df[column].astype('string')\n",
    "        elif df[column].dtype == 'category':\n",
    "            df[column] = df[column].astype('string')\n",
    "        elif df[column].dtype == 'int64':\n",
    "            df[column] = df[column].astype('int')\n",
    "        elif df[column].dtype == 'float64':\n",
    "            df[column] = df[column].astype('float')\n",
    "        elif df[column].dtype == 'datetime64[ns]':\n",
    "            df[column] = df[column].astype('datetime64[ns]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Func to Create Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a BigQuery table with partitioning if it does not exist\n",
    "def create_table_if_not_exists(table_id, schema, partition_field=None):\n",
    "    \"\"\"\n",
    "    Creates a BigQuery table if it does not exist. Supports optional schema definition \n",
    "    and time-based partitioning.\n",
    "\n",
    "    Parameters:\n",
    "    - table_id (str): The fully qualified BigQuery table ID (e.g., 'project_id.dataset_id.table_id').\n",
    "    - schema (list[google.cloud.bigquery.SchemaField], optional): The schema for the table. \n",
    "      If None, the schema will not be explicitly defined.\n",
    "    - partition_field (str, optional): The column name to use for time-based partitioning. \n",
    "      If None, no partitioning is applied.\n",
    "\n",
    "    Behavior:\n",
    "    - If the table exists and partitioning is required but not present, the table will be \n",
    "      recreated with the specified partitioning.\n",
    "    - If the table does not exist, it will be created with the specified schema and partitioning.\n",
    "\n",
    "    Raises:\n",
    "    - google.api_core.exceptions.GoogleAPICallError: If there is an error during the API call.\n",
    "    - google.api_core.exceptions.NotFound: If the specified table or dataset does not exist.\n",
    "    - google.api_core.exceptions.Conflict: If there is a conflict during table creation.\n",
    "\n",
    "    Example:\n",
    "    create_table_if_not_exists(\n",
    "        table_id=\"project_id.dataset_id.table_name\",\n",
    "        schema=fact_table_schema,\n",
    "        partition_field=\"Attrition_Flag\"\n",
    "    )\n",
    "    \"\"\"\n",
    "    try:\n",
    "        table = client.get_table(table_id)  # Make an API request.\n",
    "        if partition_field and table.time_partitioning is None:\n",
    "            print(f\"Table {table_id} exists without partitioning, but partitioning is required. Recreating the table with partitioning.\")\n",
    "            client.delete_table(table_id)  # Delete the existing table\n",
    "            table = bigquery.Table(table_id, schema=schema)\n",
    "            table.time_partitioning = bigquery.TimePartitioning(\n",
    "                type_=bigquery.TimePartitioningType.DAY,\n",
    "                field=partition_field\n",
    "            )\n",
    "            table = client.create_table(table)  # Recreate the table with partitioning\n",
    "            print(f\"Recreated table {table_id} with partitioning.\")\n",
    "        print(f\"Table {table_id} already exists.\")\n",
    "    except google.api_core.exceptions.NotFound:\n",
    "        table = bigquery.Table(table_id, schema=schema)\n",
    "        if partition_field:\n",
    "            table.time_partitioning = bigquery.TimePartitioning(\n",
    "                type_=bigquery.TimePartitioningType.DAY,\n",
    "                field=partition_field\n",
    "            )\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        print(f\"Created table {table_id}\")\n",
    "    except google.api_core.exceptions.Conflict:\n",
    "        print(f\"Table {table_id} already exists and cannot be created again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load a DataFrame into a BigQuery table with partitioning\n",
    "def load_to_bigquery(df, table_id, schema=None, partition_field=None):\n",
    "    \"\"\"\n",
    "    Loads a pandas DataFrame into a BigQuery table. If the table does not exist, it will be created.\n",
    "    Supports optional schema definition and time-based partitioning.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to load into BigQuery.\n",
    "    - table_id (str): The fully qualified BigQuery table ID (e.g., 'project_id.dataset_id.table_id').\n",
    "    - schema (list[google.cloud.bigquery.SchemaField], optional): The schema for the table. If None, schema will be inferred.\n",
    "    - partition_field (str, optional): The column name to use for time-based partitioning. If None, no partitioning is applied.\n",
    "\n",
    "    Behavior:\n",
    "    - If the table does not exist, it will be created with the specified schema and partitioning.\n",
    "    - If the table exists, its data will be replaced (WRITE_TRUNCATE mode).\n",
    "    - If partition_field is provided, the table will be partitioned by the specified column.\n",
    "\n",
    "    Raises:\n",
    "    - google.api_core.exceptions.GoogleAPICallError: If there is an error during the API call.\n",
    "    - google.api_core.exceptions.NotFound: If the specified table or dataset does not exist.\n",
    "    - google.api_core.exceptions.Conflict: If there is a conflict during table creation.\n",
    "\n",
    "    Example:\n",
    "    load_to_bigquery(\n",
    "        df=fact_table,\n",
    "        table_id=\"project_id.dataset_id.fact_table\",\n",
    "        schema=fact_table_schema,\n",
    "        partition_field=\"Attrition_Flag\"\n",
    "    )\n",
    "    \"\"\"\n",
    "    create_table_if_not_exists(table_id, schema, partition_field)\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "        schema=schema,\n",
    "        time_partitioning=bigquery.TimePartitioning(\n",
    "            type_=bigquery.TimePartitioningType.DAY,\n",
    "            field=partition_field\n",
    "        ) if partition_field else None\n",
    "    )\n",
    "    job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "    job.result()  # Wait for the job to complete\n",
    "    print(f\"Loaded {df.shape[0]} rows into {table_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Schema for the Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the fact_table DataFrame\n",
    "fact_table_schema = [\n",
    "    bigquery.SchemaField(\"Attrition_Flag\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Age\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"Gender\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Dependent_count\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"Education_Level\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Marital_Status\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Income_Category\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Card_Category\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Months_on_book\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"Total_Relationship_Count\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"Months_Inactive_12_mon\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"Contacts_Count_12_mon\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"Credit_Limit\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"Total_Revolving_Bal\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"Avg_Open_To_Buy\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"Total_Amt_Chng_Q4_Q1\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"Total_Trans_Amt\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"Total_Trans_Ct\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"Total_Ct_Chng_Q4_Q1\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"Avg_Utilization_Ratio\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"is_Attrited\", \"BOOLEAN\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Tables to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table mikecancell-development.Datasets.Banking_Data_fact_table\n",
      "Loaded 10127 rows into mikecancell-development.Datasets.Banking_Data_fact_table\n",
      "Created table mikecancell-development.Datasets.Banking_Data_dim_age\n",
      "Loaded 45 rows into mikecancell-development.Datasets.Banking_Data_dim_age\n",
      "Created table mikecancell-development.Datasets.Banking_Data_dim_contact\n",
      "Loaded 7 rows into mikecancell-development.Datasets.Banking_Data_dim_contact\n",
      "Created table mikecancell-development.Datasets.Banking_Data_dim_credit_limits\n",
      "Loaded 6205 rows into mikecancell-development.Datasets.Banking_Data_dim_credit_limits\n",
      "Created table mikecancell-development.Datasets.Banking_Data_dim_inactive\n",
      "Loaded 7 rows into mikecancell-development.Datasets.Banking_Data_dim_inactive\n",
      "Created table mikecancell-development.Datasets.Banking_Data_dim_naive_bayes\n",
      "Loaded 1591 rows into mikecancell-development.Datasets.Banking_Data_dim_naive_bayes\n",
      "Created table mikecancell-development.Datasets.Banking_Data_dim_revolving_bal\n",
      "Loaded 1974 rows into mikecancell-development.Datasets.Banking_Data_dim_revolving_bal\n",
      "Created table mikecancell-development.Datasets.Banking_Data_dim_trans_amt\n",
      "Loaded 5033 rows into mikecancell-development.Datasets.Banking_Data_dim_trans_amt\n",
      "Created table mikecancell-development.Datasets.Banking_Data_dim_trans_cnt\n",
      "Loaded 126 rows into mikecancell-development.Datasets.Banking_Data_dim_trans_cnt\n",
      "Created table mikecancell-development.Datasets.Banking_Data_dim_utilization\n",
      "Loaded 964 rows into mikecancell-development.Datasets.Banking_Data_dim_utilization\n"
     ]
    }
   ],
   "source": [
    "# Load each DataFrame into its corresponding BigQuery table without partitioning\n",
    "for df_name, df in dataframes.items():\n",
    "    table_id = f\"{dataset_id}.{table_names[df_name]}\"\n",
    "    schema = fact_table_schema if df_name == 'fact_table' else None\n",
    "    load_to_bigquery(df, table_id, schema=schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete the Tables to Save Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table deletion canceled.\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Do you really want to delete the tables? (y to proceed, n or any other key to cancel): \").strip().lower()\n",
    "\n",
    "if user_input == 'y':\n",
    "    for table_name in table_names.values():\n",
    "        table_id = f\"mikecancell-development.Datasets.{table_name}\"\n",
    "        try:\n",
    "            client.delete_table(table_id)\n",
    "            print(f\"Deleted table {table_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting table {table_id}: {e}\")\n",
    "else:\n",
    "    print(\"Table deletion canceled.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
