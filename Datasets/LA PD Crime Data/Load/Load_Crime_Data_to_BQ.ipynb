{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Conf and Credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Directory Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Common_Funcs_Dir': '/Users/mike/Develop/Projects/Code Notebook/Common/Functions', 'Credentials_Dir': '/Users/mike/Develop/Projects/Code Notebook/Credentials', 'Rel_Pickes_Dir': '../.pickles', 'Pub_Data_Dir': \"'/Users/mike/Data/Public\", 'BQ_Service_Key': '/Users/mike/Develop/Conf/GCP Service Keys/mikecancell-development-0bcca41f8486.json'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Check if the file exists and load the JSON file into a dictionary\n",
    "file_path = r'C:\\Users\\mike\\Develop\\Projects\\Code Notebook\\Credentials\\locations_conf.json'\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        locations_data = json.load(f)\n",
    "    print(locations_data)\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Pickled Dataframes into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DataFrame from zip: crime_facts\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 585000 entries, 0 to 584999\n",
      "Data columns (total 14 columns):\n",
      " #   Column          Non-Null Count   Dtype         \n",
      "---  ------          --------------   -----         \n",
      " 0   dr_no           585000 non-null  object        \n",
      " 1   date_rptd       585000 non-null  datetime64[ns]\n",
      " 2   datetime_occ    585000 non-null  datetime64[ns]\n",
      " 3   rpt_dist_no     585000 non-null  int16         \n",
      " 4   vict_age        585000 non-null  int16         \n",
      " 5   lat             585000 non-null  float64       \n",
      " 6   lon             585000 non-null  float64       \n",
      " 7   area            585000 non-null  int16         \n",
      " 8   premis_cd       585000 non-null  int16         \n",
      " 9   crm_cd          585000 non-null  int16         \n",
      " 10  vict_sex        585000 non-null  category      \n",
      " 11  vict_descent    585000 non-null  category      \n",
      " 12  weapon_used_cd  585000 non-null  int16         \n",
      " 13  status          585000 non-null  category      \n",
      "dtypes: category(3), datetime64[ns](2), float64(2), int16(6), object(1)\n",
      "memory usage: 30.7+ MB\n",
      "None\n",
      "Loaded DataFrame from zip: dim_area\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21 entries, 0 to 20\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype   \n",
      "---  ------     --------------  -----   \n",
      " 0   fk_area    21 non-null     int16   \n",
      " 1   area_name  21 non-null     category\n",
      "dtypes: category(1), int16(1)\n",
      "memory usage: 363.0 bytes\n",
      "None\n",
      "Loaded DataFrame from zip: dim_crime\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 139 entries, 0 to 138\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   fk_crm_cd    139 non-null    int16   \n",
      " 1   crm_cd_desc  139 non-null    category\n",
      "dtypes: category(1), int16(1)\n",
      "memory usage: 1.8 KB\n",
      "None\n",
      "Loaded DataFrame from zip: dim_location\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 52528 entries, 0 to 169383\n",
      "Data columns (total 16 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   lat                 52528 non-null  float64\n",
      " 1   lon                 52528 non-null  float64\n",
      " 2   geo_place_id        52528 non-null  object \n",
      " 3   geo_osm_type        52528 non-null  object \n",
      " 4   geo_osm_id          52528 non-null  object \n",
      " 5   geo_display_name    52528 non-null  object \n",
      " 6   geo_road            52416 non-null  object \n",
      " 7   geo_neighbourhood   8298 non-null   object \n",
      " 8   geo_suburb          35173 non-null  object \n",
      " 9   geo_city            52113 non-null  object \n",
      " 10  geo_state           52527 non-null  object \n",
      " 11  geo_ISO3166-2-lvl4  52527 non-null  object \n",
      " 12  geo_postcode        52364 non-null  object \n",
      " 13  geo_country         52527 non-null  object \n",
      " 14  geo_country_code    52527 non-null  object \n",
      " 15  geo_boundingbox     52528 non-null  object \n",
      "dtypes: float64(2), object(14)\n",
      "memory usage: 6.8+ MB\n",
      "None\n",
      "Loaded DataFrame from zip: dim_premise\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 310 entries, 0 to 309\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype   \n",
      "---  ------        --------------  -----   \n",
      " 0   fk_premis_cd  310 non-null    int16   \n",
      " 1   premis_desc   302 non-null    category\n",
      "dtypes: category(1), int16(1)\n",
      "memory usage: 3.7 KB\n",
      "None\n",
      "Loaded DataFrame from zip: dim_status\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   fk_status    6 non-null      category\n",
      " 1   status_desc  7 non-null      category\n",
      "dtypes: category(2)\n",
      "memory usage: 242.0 bytes\n",
      "None\n",
      "Loaded DataFrame from zip: dim_victim\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19 entries, 0 to 19\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype   \n",
      "---  ------        --------------  -----   \n",
      " 0   vict_descent  19 non-null     category\n",
      " 1   descent_desc  19 non-null     object  \n",
      "dtypes: category(1), object(1)\n",
      "memory usage: 483.0+ bytes\n",
      "None\n",
      "Loaded DataFrame from zip: dim_weapon\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 78 entries, 0 to 77\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype   \n",
      "---  ------             --------------  -----   \n",
      " 0   fk_weapon_used_cd  78 non-null     int16   \n",
      " 1   weapon_desc        77 non-null     category\n",
      "dtypes: category(1), int16(1)\n",
      "memory usage: 982.0 bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "directory = locations_data['Rel_Pickes_Dir']\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.pkl') or filename.endswith('.pkl.zip'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        try:\n",
    "            if filename.endswith('.pkl.zip'):\n",
    "                with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "                    for zip_info in zip_ref.infolist():\n",
    "                        if zip_info.filename.endswith('.pkl'):\n",
    "                            with zip_ref.open(zip_info) as file:\n",
    "                                df_name = os.path.splitext(os.path.splitext(filename)[0])[0]\n",
    "                                globals()[df_name] = pd.DataFrame(pickle.load(file))\n",
    "                                print(f\"Loaded DataFrame from zip: {df_name}\")\n",
    "                                print(globals()[df_name].info())\n",
    "            else:\n",
    "                with open(filepath, 'rb') as file:\n",
    "                    df_name = os.path.splitext(filename)[0]\n",
    "                    globals()[df_name] = pd.DataFrame(pickle.load(file))\n",
    "                    print(f\"Loaded DataFrame: {df_name}\")\n",
    "                    print(globals()[df_name].info())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "\n",
    "# Now each pickle file is loaded into its own respective DataFrame variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a connection to Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets in project mikecancell-development:\n",
      "\tDatasets\n",
      "\tVC_data_job_postings_data_api\n",
      "\tuber_data\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Path to the service account key file\n",
    "key_path = locations_data['BQ_Service_Key']\n",
    "\n",
    "# Create credentials using the service account key file\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "\n",
    "# Create a BigQuery client using the credentials\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "# Test the connection by listing datasets\n",
    "datasets = list(client.list_datasets())\n",
    "if datasets:\n",
    "    print(\"Datasets in project {}:\".format(client.project))\n",
    "    for dataset in datasets:\n",
    "        print(\"\\t{}\".format(dataset.dataset_id))\n",
    "else:\n",
    "    print(\"{} project does not contain any datasets.\".format(client.project))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset to Store the Tables (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset mikecancell-development.Datasets already exists.\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset ID\n",
    "dataset_id = \"{}.Datasets\".format(client.project)\n",
    "\n",
    "# Check if the dataset already exists\n",
    "try:\n",
    "\tclient.get_dataset(dataset_id)  # Make an API request.\n",
    "\tprint(\"Dataset {} already exists.\".format(dataset_id))\n",
    "except Exception:\n",
    "\t# Construct a full Dataset object to send to the API\n",
    "\tdataset = bigquery.Dataset(dataset_id)\n",
    "\n",
    "\t# Specify the geographic location where the dataset should reside\n",
    "\tdataset.location = \"US\"\n",
    "\n",
    "\t# Send the dataset to the API for creation\n",
    "\tdataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "\n",
    "\tprint(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Tables to the Clould"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14513.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table mikecancell-development.Datasets.LA_Crime_crime_facts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12633.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table mikecancell-development.Datasets.LA_Crime_dim_area\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25115.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table mikecancell-development.Datasets.LA_Crime_dim_crime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13751.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table mikecancell-development.Datasets.LA_Crime_dim_location\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 27594.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table mikecancell-development.Datasets.LA_Crime_dim_premise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 35544.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table mikecancell-development.Datasets.LA_Crime_dim_status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11037.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table mikecancell-development.Datasets.LA_Crime_dim_victim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5454.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table mikecancell-development.Datasets.LA_Crime_dim_weapon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Install the pandas-gbq package\n",
    "# %pip install pandas-gbq\n",
    "\n",
    "from pandas_gbq import to_gbq\n",
    "\n",
    "# Define the dataset ID\n",
    "dataset_id = \"Datasets\"\n",
    "\n",
    "# Define the table names and dataframes dynamically from the loaded DataFrames\n",
    "table_names = {df_name: f\"LA_Crime_{df_name}\" for df_name in globals() if isinstance(globals()[df_name], pd.DataFrame)}\n",
    "dataframes = {df_name: globals()[df_name] for df_name in table_names}\n",
    "\n",
    "# Ensure all columns are converted to a suitable datatype\n",
    "for df_name in dataframes:\n",
    "    dataframes[df_name] = dataframes[df_name].astype(str)\n",
    "\n",
    "# Create tables in BigQuery\n",
    "for df_name, table_name in table_names.items():\n",
    "    table_id = f\"{client.project}.{dataset_id}.{table_name}\"\n",
    "    to_gbq(dataframes[df_name], table_id, project_id=client.project, if_exists='replace', credentials=credentials)\n",
    "    print(f\"Created table {table_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete the Tables to Save Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted table mikecancell-development.Datasets.crime_facts\n",
      "Deleted table mikecancell-development.Datasets.dim_area\n",
      "Deleted table mikecancell-development.Datasets.dim_crime\n",
      "Deleted table mikecancell-development.Datasets.dim_location\n",
      "Deleted table mikecancell-development.Datasets.dim_premise\n",
      "Deleted table mikecancell-development.Datasets.dim_status\n",
      "Deleted table mikecancell-development.Datasets.dim_victim\n",
      "Deleted table mikecancell-development.Datasets.dim_weapon\n"
     ]
    }
   ],
   "source": [
    "# Delete tables in BigQuery\n",
    "for table_name in table_names.values():\n",
    "    table_id = f\"{client.project}.{dataset_id}.{table_name}\"\n",
    "    try:\n",
    "        client.delete_table(table_id)  # Make an API request.\n",
    "        print(f\"Deleted table {table_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting table {table_id}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
