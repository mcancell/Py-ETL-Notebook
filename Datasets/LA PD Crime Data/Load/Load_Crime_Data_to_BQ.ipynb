{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Conf and Credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Directory Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Common_Funcs_Dir': '/Users/mike/Develop/Projects/Code Notebook/Common/Functions', 'Credentials_Dir': '/Users/mike/Develop/Projects/Code Notebook/Credentials', 'Rel_Pickes_Dir': '../.pickles', 'Pub_Data_Dir': \"'/Users/mike/Data/Public\", 'BQ_Service_Key': '/Users/mike/Develop/Conf/GCP Service Keys/mikecancell-development-0bcca41f8486.json'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Check if the file exists and load the JSON file into a dictionary\n",
    "file_path = r'C:\\Users\\mike\\Develop\\Projects\\Code Notebook\\Credentials\\locations_conf.json'\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        locations_data = json.load(f)\n",
    "    print(locations_data)\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Pickled Dataframes into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DataFrame from zip: crime_facts\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 585000 entries, 0 to 584999\n",
      "Data columns (total 14 columns):\n",
      " #   Column          Non-Null Count   Dtype         \n",
      "---  ------          --------------   -----         \n",
      " 0   dr_no           585000 non-null  object        \n",
      " 1   date_rptd       585000 non-null  datetime64[ns]\n",
      " 2   datetime_occ    585000 non-null  datetime64[ns]\n",
      " 3   rpt_dist_no     585000 non-null  int16         \n",
      " 4   vict_age        585000 non-null  int16         \n",
      " 5   lat             585000 non-null  float64       \n",
      " 6   lon             585000 non-null  float64       \n",
      " 7   area            585000 non-null  int16         \n",
      " 8   premis_cd       585000 non-null  int16         \n",
      " 9   crm_cd          585000 non-null  int16         \n",
      " 10  vict_sex        585000 non-null  category      \n",
      " 11  vict_descent    585000 non-null  category      \n",
      " 12  weapon_used_cd  585000 non-null  int16         \n",
      " 13  status          585000 non-null  category      \n",
      "dtypes: category(3), datetime64[ns](2), float64(2), int16(6), object(1)\n",
      "memory usage: 30.7+ MB\n",
      "None\n",
      "Loaded DataFrame from zip: dim_area\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21 entries, 0 to 20\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype   \n",
      "---  ------     --------------  -----   \n",
      " 0   fk_area    21 non-null     int16   \n",
      " 1   area_name  21 non-null     category\n",
      "dtypes: category(1), int16(1)\n",
      "memory usage: 363.0 bytes\n",
      "None\n",
      "Loaded DataFrame from zip: dim_crime\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 139 entries, 0 to 138\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   fk_crm_cd    139 non-null    int16   \n",
      " 1   crm_cd_desc  139 non-null    category\n",
      "dtypes: category(1), int16(1)\n",
      "memory usage: 1.8 KB\n",
      "None\n",
      "Loaded DataFrame from zip: dim_location\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 52528 entries, 0 to 169383\n",
      "Data columns (total 16 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   lat                 52528 non-null  float64\n",
      " 1   lon                 52528 non-null  float64\n",
      " 2   geo_place_id        52528 non-null  object \n",
      " 3   geo_osm_type        52528 non-null  object \n",
      " 4   geo_osm_id          52528 non-null  object \n",
      " 5   geo_display_name    52528 non-null  object \n",
      " 6   geo_road            52416 non-null  object \n",
      " 7   geo_neighbourhood   8298 non-null   object \n",
      " 8   geo_suburb          35173 non-null  object \n",
      " 9   geo_city            52113 non-null  object \n",
      " 10  geo_state           52527 non-null  object \n",
      " 11  geo_ISO3166-2-lvl4  52527 non-null  object \n",
      " 12  geo_postcode        52364 non-null  object \n",
      " 13  geo_country         52527 non-null  object \n",
      " 14  geo_country_code    52527 non-null  object \n",
      " 15  geo_boundingbox     52528 non-null  object \n",
      "dtypes: float64(2), object(14)\n",
      "memory usage: 6.8+ MB\n",
      "None\n",
      "Loaded DataFrame from zip: dim_premise\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 310 entries, 0 to 309\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype   \n",
      "---  ------        --------------  -----   \n",
      " 0   fk_premis_cd  310 non-null    int16   \n",
      " 1   premis_desc   302 non-null    category\n",
      "dtypes: category(1), int16(1)\n",
      "memory usage: 3.7 KB\n",
      "None\n",
      "Loaded DataFrame from zip: dim_status\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   fk_status    6 non-null      category\n",
      " 1   status_desc  7 non-null      category\n",
      "dtypes: category(2)\n",
      "memory usage: 242.0 bytes\n",
      "None\n",
      "Loaded DataFrame from zip: dim_victim\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19 entries, 0 to 19\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype   \n",
      "---  ------        --------------  -----   \n",
      " 0   vict_descent  19 non-null     category\n",
      " 1   descent_desc  19 non-null     object  \n",
      "dtypes: category(1), object(1)\n",
      "memory usage: 483.0+ bytes\n",
      "None\n",
      "Loaded DataFrame from zip: dim_weapon\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 78 entries, 0 to 77\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype   \n",
      "---  ------             --------------  -----   \n",
      " 0   fk_weapon_used_cd  78 non-null     int16   \n",
      " 1   weapon_desc        77 non-null     category\n",
      "dtypes: category(1), int16(1)\n",
      "memory usage: 982.0 bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "directory = locations_data['Rel_Pickes_Dir']\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.pkl') or filename.endswith('.pkl.zip'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        try:\n",
    "            if filename.endswith('.pkl.zip'):\n",
    "                with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "                    for zip_info in zip_ref.infolist():\n",
    "                        if zip_info.filename.endswith('.pkl'):\n",
    "                            with zip_ref.open(zip_info) as file:\n",
    "                                df_name = os.path.splitext(os.path.splitext(filename)[0])[0]\n",
    "                                globals()[df_name] = pd.DataFrame(pickle.load(file))\n",
    "                                print(f\"Loaded DataFrame from zip: {df_name}\")\n",
    "                                print(globals()[df_name].info())\n",
    "            else:\n",
    "                with open(filepath, 'rb') as file:\n",
    "                    df_name = os.path.splitext(filename)[0]\n",
    "                    globals()[df_name] = pd.DataFrame(pickle.load(file))\n",
    "                    print(f\"Loaded DataFrame: {df_name}\")\n",
    "                    print(globals()[df_name].info())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "\n",
    "# Now each pickle file is loaded into its own respective DataFrame variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a connection to Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets in project mikecancell-development:\n",
      "\tDatasets\n",
      "\tVC_data_job_postings_data_api\n",
      "\tuber_data\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Path to the service account key file\n",
    "key_path = locations_data['BQ_Service_Key']\n",
    "\n",
    "# Create credentials using the service account key file\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "\n",
    "# Create a BigQuery client using the credentials\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "# Test the connection by listing datasets\n",
    "datasets = list(client.list_datasets())\n",
    "if datasets:\n",
    "    print(\"Datasets in project {}:\".format(client.project))\n",
    "    for dataset in datasets:\n",
    "        print(\"\\t{}\".format(dataset.dataset_id))\n",
    "else:\n",
    "    print(\"{} project does not contain any datasets.\".format(client.project))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset to Store the Tables (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset mikecancell-development.Datasets already exists.\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset ID\n",
    "dataset_id = \"{}.Datasets\".format(client.project)\n",
    "\n",
    "# Check if the dataset already exists\n",
    "try:\n",
    "\tclient.get_dataset(dataset_id)  # Make an API request.\n",
    "\tprint(\"Dataset {} already exists.\".format(dataset_id))\n",
    "except Exception:\n",
    "\t# Construct a full Dataset object to send to the API\n",
    "\tdataset = bigquery.Dataset(dataset_id)\n",
    "\n",
    "\t# Specify the geographic location where the dataset should reside\n",
    "\tdataset.location = \"US\"\n",
    "\n",
    "\t# Send the dataset to the API for creation\n",
    "\tdataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "\n",
    "\tprint(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Tables to the Clould"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table mikecancell-development.Datasets.LA_Crime_crime_facts exists without partitioning, but partitioning is required. Recreating the table with partitioning.\n",
      "Recreated table mikecancell-development.Datasets.LA_Crime_crime_facts with partitioning.\n",
      "Table mikecancell-development.Datasets.LA_Crime_crime_facts already exists.\n",
      "Loaded 585000 rows into mikecancell-development.Datasets.LA_Crime_crime_facts\n",
      "Table mikecancell-development.Datasets.LA_Crime_dim_area already exists.\n",
      "Loaded 21 rows into mikecancell-development.Datasets.LA_Crime_dim_area\n",
      "Table mikecancell-development.Datasets.LA_Crime_dim_crime already exists.\n",
      "Loaded 139 rows into mikecancell-development.Datasets.LA_Crime_dim_crime\n",
      "Table mikecancell-development.Datasets.LA_Crime_dim_location already exists.\n",
      "Loaded 52528 rows into mikecancell-development.Datasets.LA_Crime_dim_location\n",
      "Table mikecancell-development.Datasets.LA_Crime_dim_premise already exists.\n",
      "Loaded 310 rows into mikecancell-development.Datasets.LA_Crime_dim_premise\n",
      "Table mikecancell-development.Datasets.LA_Crime_dim_status already exists.\n",
      "Loaded 7 rows into mikecancell-development.Datasets.LA_Crime_dim_status\n",
      "Table mikecancell-development.Datasets.LA_Crime_dim_victim already exists.\n",
      "Loaded 19 rows into mikecancell-development.Datasets.LA_Crime_dim_victim\n",
      "Table mikecancell-development.Datasets.LA_Crime_dim_weapon already exists.\n",
      "Loaded 78 rows into mikecancell-development.Datasets.LA_Crime_dim_weapon\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from google.cloud import bigquery\n",
    "import google.api_core.exceptions\n",
    "\n",
    "# Update table names to precede with \"LA_Crime_\"\n",
    "table_names = {\n",
    "    'crime_facts': 'LA_Crime_crime_facts',\n",
    "    'dim_area': 'LA_Crime_dim_area',\n",
    "    'dim_crime': 'LA_Crime_dim_crime',\n",
    "    'dim_location': 'LA_Crime_dim_location',\n",
    "    'dim_premise': 'LA_Crime_dim_premise',\n",
    "    'dim_status': 'LA_Crime_dim_status',\n",
    "    'dim_victim': 'LA_Crime_dim_victim',\n",
    "    'dim_weapon': 'LA_Crime_dim_weapon'\n",
    "}\n",
    "\n",
    "# Create a dictionary to store the DataFrames and their corresponding BigQuery table names\n",
    "dataframes = {\n",
    "    'crime_facts': crime_facts,\n",
    "    'dim_area': dim_area,\n",
    "    'dim_crime': dim_crime,\n",
    "    'dim_location': dim_location,\n",
    "    'dim_premise': dim_premise,\n",
    "    'dim_status': dim_status,\n",
    "    'dim_victim': dim_victim,\n",
    "    'dim_weapon': dim_weapon\n",
    "}\n",
    "\n",
    "# Convert columns containing lists to strings\n",
    "for df_name, df in dataframes.items():\n",
    "    for column in df.columns:\n",
    "        if df[column].apply(lambda x: isinstance(x, list)).any():\n",
    "            df[column] = df[column].apply(lambda x: str(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Ensure all columns have explicit data types\n",
    "for df_name, df in dataframes.items():\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            df[column] = df[column].astype('string')\n",
    "        elif df[column].dtype == 'category':\n",
    "            df[column] = df[column].astype('string')\n",
    "        elif df[column].dtype == 'int64':\n",
    "            df[column] = df[column].astype('int')\n",
    "        elif df[column].dtype == 'float64':\n",
    "            df[column] = df[column].astype('float')\n",
    "        elif df[column].dtype == 'datetime64[ns]':\n",
    "            df[column] = df[column].astype('datetime64[ns]')\n",
    "\n",
    "# Define a function to create a BigQuery table with partitioning if it does not exist\n",
    "def create_table_if_not_exists(table_id, schema, partition_field=None):\n",
    "    try:\n",
    "        table = client.get_table(table_id)  # Make an API request.\n",
    "        if partition_field and table.time_partitioning is None:\n",
    "            print(f\"Table {table_id} exists without partitioning, but partitioning is required. Recreating the table with partitioning.\")\n",
    "            client.delete_table(table_id)  # Delete the existing table\n",
    "            table = bigquery.Table(table_id, schema=schema)\n",
    "            table.time_partitioning = bigquery.TimePartitioning(\n",
    "                type_=bigquery.TimePartitioningType.DAY,\n",
    "                field=partition_field\n",
    "            )\n",
    "            table = client.create_table(table)  # Recreate the table with partitioning\n",
    "            print(f\"Recreated table {table_id} with partitioning.\")\n",
    "        print(f\"Table {table_id} already exists.\")\n",
    "    except google.api_core.exceptions.NotFound:\n",
    "        table = bigquery.Table(table_id, schema=schema)\n",
    "        if partition_field:\n",
    "            table.time_partitioning = bigquery.TimePartitioning(\n",
    "                type_=bigquery.TimePartitioningType.DAY,\n",
    "                field=partition_field\n",
    "            )\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        print(f\"Created table {table_id}\")\n",
    "    except google.api_core.exceptions.Conflict:\n",
    "        print(f\"Table {table_id} already exists and cannot be created again.\")\n",
    "\n",
    "# Define a function to load a DataFrame into a BigQuery table with partitioning\n",
    "def load_to_bigquery(df, table_id, schema=None, partition_field=None):\n",
    "    create_table_if_not_exists(table_id, schema, partition_field)\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "        schema=schema,\n",
    "        time_partitioning=bigquery.TimePartitioning(\n",
    "            type_=bigquery.TimePartitioningType.DAY,\n",
    "            field=partition_field\n",
    "        ) if partition_field else None\n",
    "    )\n",
    "    job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "    job.result()  # Wait for the job to complete\n",
    "    print(f\"Loaded {df.shape[0]} rows into {table_id}\")\n",
    "\n",
    "# Define the schema for the crime_facts DataFrame\n",
    "crime_facts_schema = [\n",
    "    bigquery.SchemaField(\"dr_no\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"date_rptd\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"datetime_occ\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"rpt_dist_no\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"vict_age\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"lat\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"lon\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"area\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"premis_cd\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"crm_cd\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"vict_sex\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"vict_descent\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"weapon_used_cd\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"status\", \"STRING\")\n",
    "]\n",
    "\n",
    "# Load each DataFrame into its corresponding BigQuery table with partitioning on 'date_rptd' for crime_facts\n",
    "for df_name, df in dataframes.items():\n",
    "    table_id = f\"mikecancell-development.Datasets.{table_names[df_name]}\"\n",
    "    schema = crime_facts_schema if df_name == 'crime_facts' else None\n",
    "    partition_field = 'date_rptd' if df_name == 'crime_facts' else None\n",
    "    if df_name == 'crime_facts':\n",
    "        load_to_bigquery(df, table_id, schema=schema, partition_field=partition_field)\n",
    "    else:\n",
    "        load_to_bigquery(df, table_id, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete the Tables to Save Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted table mikecancell-development.Datasets.crime_facts\n",
      "Deleted table mikecancell-development.Datasets.dim_area\n",
      "Deleted table mikecancell-development.Datasets.dim_crime\n",
      "Deleted table mikecancell-development.Datasets.dim_location\n",
      "Deleted table mikecancell-development.Datasets.dim_premise\n",
      "Deleted table mikecancell-development.Datasets.dim_status\n",
      "Deleted table mikecancell-development.Datasets.dim_victim\n",
      "Deleted table mikecancell-development.Datasets.dim_weapon\n"
     ]
    }
   ],
   "source": [
    "for table_name in table_names.values():\n",
    "    table_id = f\"mikecancell-development.Datasets.{table_name}\"\n",
    "    try:\n",
    "        client.delete_table(table_id)\n",
    "        print(f\"Deleted table {table_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting table {table_id}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
