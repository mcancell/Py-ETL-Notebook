{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Info\n",
    "The project is currently under development and changes regularly both in terms of code and organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Me \n",
    "\n",
    "My name is Mike Cancell (https://www.linkedin.com/in/mikecancell/). If you’re visiting this repository, it’s likely because you want to evaluate my current Python skills. This repo will give you some insight into my capabilities. \n",
    "\n",
    "I’m still a novice in Python, but I’m learning more each day. On the other hand, I possess strong SQL skills. If you're interested in reviewing my previous SQL work, please reach out to me via LinkedIn. \n",
    "\n",
    "While I don't have a public repository for my SQL work due to it being actual in-production code, I’m willing to share some code samples if you're seriously considering hiring me. The shared code will not contain any sensitive data, so you’ll be able to see how I model financial data and process KPIs without exposing any actual company information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Local Dev Env\n",
    "While I may move code back to my cloud VM, I am currently working locally on a Windows laptop to avoid costs. I do maintain a cloud instance on GCP, but it is currently idle to minimize monthly expenses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Env Requirements\n",
    "See the Requirements.txt file for the list of lib dependencies. (I try to Freeze the Reqs file regularly.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Deployment\n",
    "Right now, I am not using Docker, Kubernetes, etc. for code deployment. I may try that as an excercise in the future. But as of now, I am just using VS Code with a local env."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Coding\n",
    "I’m a Python novice/journeyman ;-). While I code SQL by hand without any AI assistance, I am now using AI—specifically Co-Pilot—for Python, and I absolutely love it! \n",
    "\n",
    "Has AI changed my coding experience? Yes, it has. It's helping me quickly navigate the vast array of Python libraries. I'm new to using AI for coding; I used to spend a lot of time hand-coding in JavaScript and other non-native languages. SQL is my native language, although I was once a C programmer at Bell Labs—back in the horse-and-buggy days (don't ask...).\n",
    "\n",
    "Here’s the takeaway: using AI is not a crutch, in my opinion. You still need to understand programming tasks to ask the right questions. When it comes to Python, that's exactly what I focus on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Goals\n",
    "The goal of the project is to demonstrate the typical types of pipelines and workflows that are performed when working with diffent types of Datasets. \n",
    "Thus, while there are some algorythimic examples within the different pipelines, the main purpose about working with data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common aspects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallization\n",
    "You will find that in many pipelines, parallization is used to increase performance of certain operations, e.g., web scraping. Please see the concurrent.futures lib data re: Launching parallel tasks -https://docs.python.org/3/library/concurrent.futures.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Optimization\n",
    "Whenever appropriate, buffering is used so that Dataframe loading does not hit memory issues. For example, in the Taxi Trip Dataset, it is possible to load >1M rows without running out of memory. Sometimes, I use Pandas, and sometimes I use Polars. But regardless, attention is paid to potential memory issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress and Status Reporting\n",
    "The tqdm (https://pypi.org/project/tqdm/) lib is used for Progress Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching \n",
    "Pickle Files (https://docs.python.org/3/library/pickle.html) are used to cache dataframes. They are used to both to prevent re-calling operations, e.g., whois, screenscraping meta tags, etc. and they are used for downstream workflows, e.g., Load Data from Internet (JSON) ...and pickle, Clean and Transformdata ...and pickle, Load db tables, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Compression\n",
    "Insofar as large datasets are ingested from Internet, local data is often stored in compressed format, e.g., zip. Likewise, when reading, data is often read from within the compressed file (e.g., w/out decompressing first)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and History\n",
    "I created this repo in Dec 2024. It was around that time that I got caught up in a force reduction at my former company, Interactions Corp. This repo basically represents my journey to learn how to utilize Python for ETL and ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Direction\n",
    "After I exhaust each dataset, in terms of what could be analyzed from the data. I move on to another dataset. But I am confined to *free* sources, iow I am not paying for data, e.g., D&B, etc. So all datasets are of the 'Beg, Borrow, and Steal' variety. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orchestration and Deployment\n",
    "As of date of this writing, I am not using any orchestration tools (Airflow, NiFi, Dagster, etc.) I hope to start adding workflow deployment in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Organization\n",
    "Project is organized by Dataset, with set having directories for Extract, Transform, Load, (and Orchestrate tbd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common\n",
    "This area contains common code, credentials, etc. which pertain across all Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "As I develop new functions that are used within a given Dataset, I will be abstracting and migrating to this area, as apprpropriate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credentials\n",
    "This area contains credential data like API keys, IAM credentials, logon credentials, etc. There is a .gitignore to prevent any sensitive credentials from getting upload to the repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .Pickles\n",
    "I use pickle files for local caching. .pkl files are ignored by Git and thus not uploaded.\n",
    "(If your not familiar with pickle usage or rationale, please see https://docs.python.org/3/library/pickle.html for details.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Operations for Each Dataset are self-contain contained with subdirectories for typlical ETL Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Dir\n",
    "This area will be used for functions, classes, etc that may be specific to the respective Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credentials\n",
    "Dataset specific Credentials may be stored here or off the project's root. But note, all credential data is git ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .pickles\n",
    "This area is used to cache dataframes specific to the respective Dataset. Note again, that all .pkl files are git ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract\n",
    "This area is where Data ingestion routines are stored. This corresponds to Extract pipelines used to ingest data from various sources.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform\n",
    "This area contains the various pipelines for data cleaning, arrangement (e.g., semantic modeling, schema), ML learning for (time series regression, categorization, entity resolution, etc.)\n",
    "Once Dataframes are fully formulared they are pickled for caching and downstream operations, e.g., loading to a cloud DB, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load\n",
    "This area containes the pipelines for loading data to a landing area, e.g., clould tables, datamart, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query\n",
    "This area contains SQL code that works on the Loaded Tables to Materialize Datasets for ultimate reporting/visulaization. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
